# -*- coding: utf-8 -*-
"""Capstone project - NLP chatbot for industrial safety.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CDHLWzgkEJOiFlmUq-Y3ND_aUDhdpYmf
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import f
from statsmodels.stats.proportion import proportions_ztest
import nltk
from sklearn.preprocessing import LabelBinarizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud, STOPWORDS
from nltk.stem import WordNetLemmatizer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize,sent_tokenize
import re,string,unicodedata
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.model_selection import train_test_split
import keras
from keras.models import Sequential
import tensorflow as tf

import holoviews as hv
from holoviews import dim, opts

hv.extension('bokeh', 'matplotlib')


# Deep learning libraries
from keras.utils import np_utils
# from keras.utils import plot_model
from keras.layers import Input
from keras.layers.merge import Concatenate
# from keras.optimizers import SGD
from tensorflow.keras.models import Sequential
from tensorflow.keras import optimizers
from keras.models import Model
from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Embedding, Dropout, Flatten, Bidirectional, GlobalMaxPool1D
from keras.models import model_from_json
from keras.regularizers import l1, l2, l1_l2
from keras.constraints import unit_norm
from keras.callbacks import ReduceLROnPlateau
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from keras.models import model_from_json


## Importing the data

nlp_data = pd.read_csv("C:\\Users\\Admin\\Documents\\chatbot-deployment\\capstone\\Data Set - industrial_safety_and_health_database_with_accidents_description.csv",encoding = 'ISO-8859-1')

nlp_data.head()

##Checking the shape of the data

nlp_data.shape

## The csv has 426 rows and 12 columns

## checking the type of data

nlp_data.info()

## We see that the dataset has mostly data type of object which is a combination of both numbers and strings.

##Checking for null value

nlp_data.isnull().sum()

nlp_data.isnull().sum().sum()

## We can see that the data does not have any null values

# Check unique values of all columns except 'Description' column
for x in nlp_data.columns:
    if x != 'Description':
      print('--'*30); print(f'Unique values of "{x}" column'); print('--'*30)
      print(nlp_data[x].unique())
      print('\n')

## We have columns of Date. The next step is to transform the date column datatype to date type and map it to consecutive months . 
## Post this, during EDA we can check using scatter plot if there is any relation between the month and occurence of accident

nlp_data["Data"] = pd.to_datetime(nlp_data['Data'])

nlp_data["Data"].dtypes

nlp_data.dtypes

## We see that the column od Date is now converted to Date time. Next we will map it to the successive months

nlp_data["Year"] = nlp_data["Data"].dt.year
nlp_data["Month"] = nlp_data["Data"].dt.month
nlp_data["Day"] = nlp_data["Data"].dt.day_name()
nlp_data

### convert months to season

##define a function to convert months to season
def month2seasons(x):
    if x in [9, 10, 11]:
        season = 'Spring'
    elif x in [12, 1, 2]:
        season = 'Summer'
    elif x in [3, 4, 5]:
        season = 'Autumn'
    elif x in [6, 7, 8]:
        season = 'Winter'
    return season

## Converting month to season

nlp_data["Season "] = nlp_data["Month"].apply(month2seasons)

nlp_data

# Check duplicates in a data frame
nlp_data.duplicated().sum()

## since we have anonymized data for countries and local along with an unnamed column at first, we can drop these columns as they dont add value

nlp_data = nlp_data.drop(["Local"], axis =1)
nlp_data = nlp_data.drop(["Countries"], axis =1)
nlp_data = nlp_data.drop(["Data"], axis =1)
# nlp_data = nlp_data.drop("Unnamed: 0", axis=1, inplace=True)

nlp_data

## We have removed unnecessary columns

"""## EDA

* Target Variable (y) : Accident Level , Potential Accident Level
* Predictor/ Independent variables (X) : 
   * Temporal - Year, Season, Month, Day
   * Categorial - Gender, Industry Sector , Critical Risk
   * NLP analysis related - Description
"""

import warnings

# Univariate Analysis

total_row_count = nlp_data.shape[0]

Mining_cnt = nlp_data[nlp_data["Industry Sector"] == 'Mining'].shape[0]
Metal_cnt = nlp_data[nlp_data["Industry Sector"]=="Metals"].shape[0]
Other_cnt = nlp_data[nlp_data["Industry Sector"]=="Others"].shape[0]

percentage_Mining = (Mining_cnt/total_row_count) * 100
percentage_Metal = (Metal_cnt/total_row_count) * 100
percentage_Others = (Other_cnt/total_row_count) * 100

print("Proportion of Mining industry is", percentage_Mining, "%" )
print("Proportion of Metal industry is", percentage_Metal, "%" )
print("Proportion of Others industry is", percentage_Others, "%" )


sector_cnt = np.round(nlp_data['Industry Sector'].value_counts(normalize=True) * 100)

hv.Bars(sector_cnt).opts(title="Industry Sector Count", color="#8888ff", xlabel="Sectors", ylabel="Percentage", yformatter='%d%%')\
                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))\
                * hv.Text('Mining', 15, f"{int(sector_cnt.loc['Mining'])}%")\
                * hv.Text('Metals', 15, f"{int(sector_cnt.loc['Metals'])}%")\
                * hv.Text('Others', 15, f"{int(sector_cnt.loc['Others'])}%")

print('--'*30); print('Value Counts for `Accident Level` label'); print('--'*40)

I_acc_cnt = nlp_data[nlp_data['Accident Level'] == 'I'].shape[0]
II_acc_cnt = nlp_data[nlp_data['Accident Level'] == 'II'].shape[0]
III_acc_cnt = nlp_data[nlp_data['Accident Level'] == 'III'].shape[0]
IV_acc_cnt = nlp_data[nlp_data['Accident Level'] == 'IV'].shape[0]
V_acc_cnt = nlp_data[nlp_data['Accident Level'] == 'V'].shape[0]
VI_acc_cnt = nlp_data[nlp_data['Accident Level'] == 'VI'].shape[0]

print(f'Accident Level - I count: {I_acc_cnt} i.e. {round(I_acc_cnt/total_row_count*100, 0)}%')
print(f'Accident Level - II count: {II_acc_cnt} i.e. {round(II_acc_cnt/total_row_count*100, 0)}%')
print(f'Accident Level - III count: {III_acc_cnt} i.e. {round(III_acc_cnt/total_row_count*100, 0)}%')
print(f'Accident Level - IV count: {IV_acc_cnt} i.e. {round(IV_acc_cnt/total_row_count*100, 0)}%')
print(f'Accident Level - V count: {V_acc_cnt} i.e. {round(V_acc_cnt/total_row_count*100, 0)}%')
print(f'Accident Level - VI count: {VI_acc_cnt} i.e. {round(VI_acc_cnt/total_row_count*100, 0)}%')

print('--'*30); print('Value Counts for `Potential Accident Level'); print('--'*40)

I_pot_acc_cnt = nlp_data[nlp_data['Potential Accident Level'] == 'I'].shape[0]
II_pot_acc_cnt = nlp_data[nlp_data['Potential Accident Level'] == 'II'].shape[0]
III_pot_acc_cnt = nlp_data[nlp_data['Potential Accident Level'] == 'III'].shape[0]
IV_pot_acc_cnt = nlp_data[nlp_data['Potential Accident Level'] == 'IV'].shape[0]
V_pot_acc_cnt = nlp_data[nlp_data['Potential Accident Level'] == 'V'].shape[0]
VI_pot_acc_cnt = nlp_data[nlp_data['Potential Accident Level'] == 'VI'].shape[0]

print(f'Potential Accident Level - I count: {I_pot_acc_cnt} i.e. {round(I_pot_acc_cnt/total_row_count*100, 0)}%')
print(f'Potential Accident Level - II count: {II_pot_acc_cnt} i.e. {round(II_pot_acc_cnt/total_row_count*100, 0)}%')
print(f'Potential Accident Level - III count: {III_pot_acc_cnt} i.e. {round(III_pot_acc_cnt/total_row_count*100, 0)}%')
print(f'Potential Accident Level - IV count: {IV_pot_acc_cnt} i.e. {round(IV_pot_acc_cnt/total_row_count*100, 0)}%')
print(f'Potential Accident Level - V count: {V_pot_acc_cnt} i.e. {round(V_pot_acc_cnt/total_row_count*100, 0)}%')
print(f'Potential Accident Level - VI count: {VI_pot_acc_cnt} i.e. {round(VI_pot_acc_cnt/total_row_count*100, 0)}%')

print('--'*30); print('Distributon of `Accident Level` & `Potential Accident Level` label'); print('--'*40)

ac_level_cnt = np.round(nlp_data['Accident Level'].value_counts(normalize=True) * 100)
pot_ac_level_cnt = np.round(nlp_data['Potential Accident Level'].value_counts(normalize=True) * 100, decimals=1)
ac_pot = pd.concat([ac_level_cnt, pot_ac_level_cnt], axis=1,sort=False).fillna(0).rename(columns={'Accident Level':'Accident', 'Potential Accident Level':'Potential'})
ac_pot = pd.melt(ac_pot.reset_index(), ['index']).rename(columns={'index':'Severity', 'variable':'Levels'})

hv.Bars(ac_pot, ['Severity', 'Levels'], 'value').opts(opts.Bars(title="Accident Levels Count", width=700, height=300,tools=['hover'],\
                                                                show_grid=True,xrotation=45, ylabel="Percentage", yformatter='%d%%'))

"""* The percentage of accidents decreases as Accident level increases
* The percentage of accidents increases as Potential Accident level increases
"""

print('--'*30); print('Value Counts for `Genre` label'); print('--'*30)

Male_cnt = nlp_data[nlp_data['Genre'] == 'Male'].shape[0]
Female_cnt = nlp_data[nlp_data['Genre'] == 'Female'].shape[0]

print(f'Male count: {Male_cnt} i.e. {round(Male_cnt/total_row_count*100, 0)}%')
print(f'Female count: {Female_cnt} i.e. {round(Female_cnt/total_row_count*100, 0)}%')

print('--'*30); print('Distributon of `Genre` label'); print('--'*30)

Genre_cnt = np.round(nlp_data['Genre'].value_counts(normalize=True) * 100)

hv.Bars(Genre_cnt).opts(title="Genre Count", color="#8888ff", xlabel="Genre", ylabel="Percentage", yformatter='%d%%')\
                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))

"""* Proportion of male population working is more than female counterparts"""

print('--'*30); print('Value Counts for `Employee or Third Party` label'); print('--'*30)

third_party_cnt = nlp_data[nlp_data['Employee or Third Party'] == 'Third Party'].shape[0]
emp_cnt = nlp_data[nlp_data['Employee or Third Party'] == 'Employee'].shape[0]
third_rem_cnt = nlp_data[nlp_data['Employee or Third Party'] == 'Third Party (Remote)'].shape[0]

print(f'Third Party count: {third_party_cnt} i.e. {round(third_party_cnt/total_row_count*100, 0)}%')
print(f'Employee count: {emp_cnt} i.e. {round(emp_cnt/total_row_count*100, 0)}%')
print(f'Third Party (Remote) count: {third_rem_cnt} i.e. {round(third_rem_cnt/total_row_count*100, 0)}%')

print('--'*30); print('Distributon of `Employee or Third Party` label'); print('--'*30)

emp_type_cnt = np.round(nlp_data['Employee or Third Party'].value_counts(normalize=True) * 100)

hv.Bars(emp_type_cnt).opts(title="Employee or Third Party Count", color="#8888ff", xlabel="Employee or Third Party", ylabel="Percentage", yformatter='%d%%')\
                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))

"""* Large number of employees are employed by third part or are directly employed
* Remote workers are less in number
"""

cr_risk_cnt = np.round(nlp_data['Critical Risk'].value_counts(normalize=True) * 100)

hv.Bars(cr_risk_cnt[::-1]).opts(title="Critical Risk Count", color="#8888ff", xlabel="Critical Risks", ylabel="Percentage", xformatter='%d%%')\
                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))

"""* Most of the cfritical risks are categorized as others
* It can also state that reasons for accidents are difficult to explain and takes time to identify precisely
"""

year_cnt = np.round(nlp_data['Year'].value_counts(normalize=True,sort=False) * 100)
year = hv.Bars(year_cnt).opts(title="Year Count", color="yellow", xlabel="Years")

month_cnt = np.round(nlp_data['Month'].value_counts(normalize=True,sort=False) * 100)
month = hv.Bars(month_cnt).opts(title="Month Count", color="#8888ff", xlabel="Months") * hv.Curve(month_cnt).opts(color='red', line_width=3)

(year + month).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True, ylabel="Percentage", yformatter='%d%%')).cols(2)

day_cnt = np.round(nlp_data['Day'].value_counts(normalize=True,sort=False) * 100)
hv.Bars(day_cnt).opts(title="Day Count", color="#8888ff", xlabel="Days") * hv.Curve(day_cnt).opts(width=500, height=300, color='red', line_width=3)

"""## Bivariate Analysis and hypothesis testing

* Bivariate analysis will be tagrtetted at analysing the relationship between the independent and the depenedent variable as follows :

  * Dependent Variable :Accident Level
  * Independent Variables : 
      * Year, Month, Weekday
      * Gender
      * Industry Type
      * Critical Risk
      * Employment Type
      
* Apart from this, we will also the the following relation :
  
  * Does gender of workforce statistically vary by industry
  * Does critical risk vary by industry
  * Does the employment vary by industry
"""

## Visual analysis of whether Accident level is differing by year

sns.countplot(x = "Year", data = nlp_data,hue = "Accident Level", palette ="Set3")

"""* Observation : By visual analysis , we can see that the proportion of all categories of accidents has been more in year 2016 as compared to 2017. We are drawing the following hypotheis:

* H0 = Proprotion of accidents in 2016 and 2017 are equal
* H1 = Proportion of accidents in 2016 and 2017 is not equal 
* Significance level = 0.05
* Test Statistics to be used : Two proportion T test
"""

f = lambda x : np.round(x/x.sum() * 100)

acc_lvl = nlp_data.groupby(['Year', 'Accident Level'])["Accident Level"].count().unstack().apply(f, axis=1).fillna(0)

print("percentage of accident Level 1 in 2016 is", acc_lvl['I'][:1])
print("percentage of accident Level 1 in 2017 is", acc_lvl['I'][1:])


acc_lvl

# Rename 'Data', 'Countries', 'Genre', 'Employee or Third Party' columns in Data frame
nlp_data.rename(columns={'Accident Level':'Accident', 'Genre':'Gender', 'Employee or Third Party':'Employeetype', "Critical Risk":"CriticalRisk","Potential Accident Level":"PotentialAccident"}, inplace=True)
nlp_data.head()

"""* Visual Analysis of whether employee type vary by Gender"""

f = lambda x : np.round(x/x.sum() * 100)
em_gen = nlp_data.groupby(['Gender','Employeetype'])['Employeetype'].count().unstack().apply(f, axis=1)

hv.Bars(pd.melt(em_gen.reset_index(), ['Gender']), ['Gender','Employeetype'], 'value').opts(opts.Bars(title="Employeetype by Gender Count", width=800, height=300,tools=['hover'],\
                                                                show_grid=True,xrotation=0, ylabel="Percentage", yformatter='%d%%'))

"""* Observations

  * Proportion of third party employees in each gender is equal.
  * Proportion of third party(remote) employees in each gender is not equal.
  * Proportion of own employees in each gender is not equal. But let's check is that difference is statistically significant?

* 1. State the H0 and Ha
* Ho = The proportions of own employees in each gender is equal.
* Ha = The proportions of own employees in each gender is not equal.
* 2. Decide the significance level: alpha = 0.05
* 3. Identify the test-statistic: Z-test of proportions
* 4. Calculate the p_value using test-statistic¶
"""

male_emp = nlp_data[nlp_data['Employeetype'] == 'Employee'].Gender.value_counts()[0]
female_emp = nlp_data[nlp_data['Employeetype'] == 'Employee'].Gender.value_counts()[1]

print([male_emp, female_emp], [Male_cnt, Female_cnt])
print(f'Proportion of own Employeetypes in male, female = {round(170/396,2)}%, {round(8/22,2)}% respectively')

t_statistic, p_value = proportions_ztest([male_emp, female_emp], [Male_cnt, Female_cnt])

print("t_statistic", t_statistic)
print("p_value", p_value)

reject_null = False
if p_value < 0.05:
    reject_null = True 
else: 
    reject_null = False
    
print("reject null? : " + str(reject_null))

"""* Hence we fail to reject Null Hypothesis, we have enough (95%) evidence to prove that, the proportion of own employees in each gender is equal.

#### Industry Sector by Gender - Is the distribution of industry sector differ significantly in different genders?
"""

f = lambda x : np.round(x/x.sum() * 100)
em_gen = nlp_data.groupby(['Gender','Industry Sector'])['Industry Sector'].count().unstack().apply(f, axis=1)

hv.Bars(pd.melt(em_gen.reset_index(), ['Gender']), ['Gender','Industry Sector'], 'value').opts(opts.Bars(title="Industry Sector by Gender Count", width=800, height=300,tools=['hover'],\
                                                                show_grid=True,xrotation=0, ylabel="Percentage", yformatter='%d%%'))

"""### Observations

* Proportion of Metals sector employees in each gender is not equal.
* Proportion of Mining sector employees in each gender is not equal.
* Proportion of Others sector employees in each gender is not equal.

#### Accident Levels by Employee type - Is the distribution of accident levels and potential accident levels differ significantly in different employee types?
"""

f = lambda x : np.round(x/x.sum() * 100)

ac_em = nlp_data.groupby(['Employeetype','Accident'])['Accident'].count().unstack().apply(f, axis=1)
ac = hv.Bars(pd.melt(ac_em.reset_index(), ['Employeetype']), ['Employeetype','Accident'], 'value').opts(opts.Bars(title="Accident by Employeetype Count"))

pot_ac_em = nlp_data.groupby(['Employeetype','PotentialAccident'])['PotentialAccident'].count().unstack().apply(f, axis=1)
pot_ac = hv.Bars(pd.melt(pot_ac_em.reset_index(), ['Employeetype']), ['Employeetype','PotentialAccident'], 'value').opts(opts.Bars(title="Potential Accident by Employeetype Count"))

(ac + pot_ac).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=0, ylabel="Percentage", yformatter='%d%%',fontsize={'title':9}))

"""#### Observations

* For both accident levels, the incidence of Employee is higher at low accident levels, but the incidence of Third parties seems to be slightly higher at high accident levels
"""

## Visual analysis of whether Accident levels is differing by gender

sns.countplot(x = "Accident", data = nlp_data,hue = "Gender", palette ="Set3")

"""* Observation : By visual analysis , we can see that the proportion of all categories of accidents are faced more by MAles as compared to females. We are drawing the following hypotheis:

* H0 = Proprotion of accidents amongnst males and females are equal
* H1 = Proportion of accidents amongnst males and females are not equal 
* Significance level = 0.05
* Test Statistics to be used : Two proportion Z test
"""

## Calculating proportion of Accidents level I faced by males :

male_emp = nlp_data[nlp_data['Accident'] == 'I'].Gender.value_counts()[0]
female_emp = nlp_data[nlp_data['Accident'] == 'I'].Gender.value_counts()[1]

print([male_emp, female_emp], [Male_cnt, Female_cnt])
print(f'Proportion of Accident I in male, female = {round(298/403,2)}%, {round(18/22,2)}% respectively')

### T test 


z_stat, pval = proportions_ztest([male_emp, female_emp] , [Male_cnt, Female_cnt])
print(z_stat)
if pval < 0.05:
    print(f'With a p-value of {round(pval,4)} the difference is significant. aka |We reject the null|')
else:
    print(f'With a p-value of {round(pval,4)} the difference is not significant. aka |We fail to reject the null|')

"""* CONCLUSION :
  * at 95% confidence level, we have enough evidence to prove that the accident level I faced by each Gender group is same. 
  * There is no difference between accident levels I faced by Males and Females
"""

## Calculating proportion of Accidents level II faced by males :

male_emp = nlp_data[nlp_data['Accident'] == 'II'].Gender.value_counts()[0]
female_emp = nlp_data[nlp_data['Accident'] == 'II'].Gender.value_counts()[1]

print([male_emp, female_emp], [Male_cnt, Female_cnt])
print(f'Proportion of Accident Level I in male, female = {round(37/403,2)}%, {round(3/22,2)}% respectively')

### T test 
from scipy.stats import f
from statsmodels.stats.proportion import proportions_ztest

z_stat, pval = proportions_ztest([male_emp, female_emp] , [Male_cnt, Female_cnt])
print(z_stat)
if pval < 0.05:
    print(f'With a p-value of {round(pval,4)} the difference is significant. aka |We reject the null|')
else:
    print(f'With a p-value of {round(pval,4)} the difference is not significant. aka |We fail to reject the null|')

"""* CONCLUSION :
  * at 95% confidence level, we have enough evidence to prove that the accident level II faced by each Gender group is same. 
  * There is no difference between accident levels II faced by Males and Females
"""

## Calculating proportion of Accidents level III faced by males :

male_emp = nlp_data[nlp_data['Accident'] == 'III'].Gender.value_counts()[0]
female_emp = nlp_data[nlp_data['Accident'] == 'III'].Gender.value_counts()[1]

print([male_emp, female_emp], [Male_cnt, Female_cnt])
print(f'Proportion of Accident Level I in male, female = {round(30/403,2)}%, {round(1/22,2)}% respectively')

### T test 
from scipy.stats import f
from statsmodels.stats.proportion import proportions_ztest

z_stat, pval = proportions_ztest([male_emp, female_emp] , [Male_cnt, Female_cnt])
print(z_stat)
if pval < 0.05:
    print(f'With a p-value of {round(pval,4)} the difference is significant. aka |We reject the null|')
else:
    print(f'With a p-value of {round(pval,4)} the difference is not significant. aka |We fail to reject the null|')

"""* CONCLUSION :
  * at 95% confidence level, we have enough evidence to prove that the accident level III faced by each Gender group is same. 
  * There is no difference between accident levels III faced by Males and Females
"""

## Calculating proportion of Accidents level III faced by males :

male_emp = nlp_data[nlp_data['Accident'] == 'IV'].Gender.value_counts()[0]


print([male_emp, female_emp], [Male_cnt, Female_cnt])
print(f'Proportion of Accident Level I in male, female = {round(30/403,2)}%, {round(1/22,2)}% respectively')

### T test 
from scipy.stats import f
from statsmodels.stats.proportion import proportions_ztest

z_stat, pval = proportions_ztest([male_emp, female_emp] , [Male_cnt, Female_cnt])
print(z_stat)
if pval < 0.05:
    print(f'With a p-value of {round(pval,4)} the difference is significant. aka |We reject the null|')
else:
    print(f'With a p-value of {round(pval,4)} the difference is not significant. aka |We fail to reject the null|')

## Calculating proportion of Accidents level III faced by males :

male_emp = nlp_data[nlp_data['Accident'] == 'V'].Gender.value_counts()[0]

print([male_emp, female_emp], [Male_cnt, Female_cnt])
print(f'Proportion of Accident Level I in male, female = {round(8/403,2)}% respectively')

### T test 
from scipy.stats import f
from statsmodels.stats.proportion import proportions_ztest

z_stat, pval = proportions_ztest([male_emp, female_emp] , [Male_cnt, Female_cnt])
print(z_stat)
if pval < 0.05:
    print(f'With a p-value of {round(pval,4)} the difference is significant. aka |We reject the null|')
else:
    print(f'With a p-value of {round(pval,4)} the difference is not significant. aka |We fail to reject the null|')

acc_level = nlp_data.groupby('Year')['Accident'].agg(['count'])

Total_acc = nlp_data.shape[0]
acc_level.head()

Total = acc_level['count'].sum()

perc_acc = (acc_level['count']/Total_acc)*100
perc_acc

### T test 
from scipy.stats import f
from statsmodels.stats.proportion import proportions_ztest

z_stat, pval = proportions_ztest([67.05,32.94],[Total_acc])
print(z_stat)
if pval < 0.05:
    print(f'With a p-value of {round(pval,4)} the difference is significant. aka |We reject the null|')
else:
    print(f'With a p-value of {round(pval,4)} the difference is not significant. aka |We fail to reject the null|')

"""* We can see that there is significant difference between 2016 and 2017

### Hypothesis testing for Industry type and accident level
"""

## Visual analysis of whether Accident levels is differing by industry

sns.countplot(x = "Industry Sector", data = nlp_data,hue = "Accident", palette ="Set3")

"""#### Observation :
* 1. Accident Severity with level 1 is happening more across all the industries as compared to others. 
* 2. Accident severity with level V is abasent in other industry category, that means, Severe accidents are restricted to metal and mining industries

### hypothesis Testing
* 1. H0 = The proportion of accidents across all the industries does not differ significantly
* 2. H1 = The proportion of accidents across all industries differ significantly

* significance level = 0.05
* Test statistics = Chi Square (Since it is proortion of accidents in more than two categories)
"""

Accident_mining = nlp_data[nlp_data['Industry Sector'] == 'Mining'].Accident.value_counts()  # number of accidents in mining
Accident_metal = nlp_data[nlp_data['Industry Sector'] == 'Metals'].Accident.value_counts()   #number of accidents in metal
Accident_others = nlp_data[nlp_data['Industry Sector'] == 'Others'].Accident.value_counts()  # number of accidents in other industries

Total_acc_mining = Accident_mining.sum()
Total_acc_metal = Accident_metal.sum()
Total_acc_others = Accident_others.sum()

from scipy.stats import chisquare
test_stat = chisquare([Total_acc_mining, Total_acc_metal, Total_acc_others])
print(test_stat)
if pval < 0.05:
    print(f'With a p-value of {round(pval)} the difference is significant. aka |We reject the null|')
else:
    print(f'With a p-value of {round(pval)} the difference is not significant. aka |We fail to reject the null|')

"""#### Conclusion

* Since the p value is less than the chi square test statitstics,we will reject null hypothesis. 
* The number of accidents differ by each industry sector

#### Accident Levels by Month - Is the distribution of accident levels and potential accident levels differ significantly in different months?
"""

f = lambda x : np.round(x/x.sum() * 100)

ac_mo = nlp_data.groupby(['Month','Accident'])['Accident'].count().unstack().apply(f, axis=1).fillna(0)
ac = hv.Curve(ac_mo['I'], label='I') * hv.Curve(ac_mo['II'], label='II') * hv.Curve(ac_mo['III'], label='III') * hv.Curve(ac_mo['IV'], label='IV') * hv.Curve(ac_mo['V'], label='V')\
        .opts(opts.Curve(title="Accident by Month Count"))

pot_ac_mo = nlp_data.groupby(['Month','PotentialAccident'])['PotentialAccident'].count().unstack().apply(f, axis=1).fillna(0)
pot_ac = hv.Curve(pot_ac_mo['I'], label='I') * hv.Curve(pot_ac_mo['II'], label='II') * hv.Curve(pot_ac_mo['III'], label='III') * hv.Curve(pot_ac_mo['IV'], label='IV')\
        * hv.Curve(pot_ac_mo['V'], label='V') * hv.Curve(pot_ac_mo['VI'], label='VI').opts(opts.Curve(title="Potential Accident by Month Count"))
        
(ac+pot_ac).opts(opts.Curve(width=800, height=300,tools=['hover'],show_grid=True, ylabel="Percentage", yformatter='%d%%')).cols(1)

"""#### Observations
** Both of the two accident level have the tendency that non-severe levels decreased throughout the year, but severe levels did not changed much, and some of these levels increased slightly in the second half of the year.

#### Study Summary Statistics
"""

# Summary statistics
nlp_data.drop(columns='Description').describe(exclude=[np.number]).T

"""## NLP ANALYSIS"""

## Next we will tackle the description column. We will tokenize the description column,remove stopwords, extract key words and see frequency of such key words

## Preprocessing the text

##Changing to lower case and substituiting numeric and special characters with nothing

nlp_data['Description'] = nlp_data['Description'].apply(lambda x: x.lower())
nlp_data['Description'] = nlp_data['Description'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))

nlp_data["Description"]

## For example, now sentence indexed with number 4 has 11:45am, written as 1145 am. There is no capital letters.

# Defining a function for NLP preprocessing
def des_cleaning(text):

    # Initialize the object for Lemmatizer class
    lemmatizer = nltk.stem.WordNetLemmatizer()

    # Set the stopwords to English
    stop_words = nltk.corpus.stopwords.words('english')

    # Normalize the text in order deal with accented words and unicodes
    text = (unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore').lower())

    # Consider only alphabets and numbers from the text
    words = re.sub(r'[^a-zA-Z.,!?/:;\"\'\s]', '', text).split()

    # Consider the words which are not in stopwords of english and lemmatize them
    lemmatizer = nltk.stem.WordNetLemmatizer()
    lems = [lemmatizer.lemmatize(i) for i in words if i not in stop_words]

    # #remove non-alphabetical characters like '(', '.' or '!')
    # alphas = [i for i in lems if (i.isalpha() or i.isnumeric()) and (i not in stopwords)]

    words = [w for w in lems if len(w)>2]

    return words

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

# Apply all the preprocessing techniques we have defined on Description columns
tokens = des_cleaning(' '.join(nlp_data['Description'].sum().split()))

nlp_data["Description"]

# Verifying by printing first 30 words

print("length of total tokens",len(tokens))
print("length of total unique tokens",len(np.unique(tokens)))
np.array(tokens[0:20])

"""#### OBSERVATION:
* 1. Total words in Description column is 13214
* 2. Total number of unqiue tokens is 3029
* 3. Only 23% of the words are unique in the description column
* 4. There is a high repetition of words in the accident description, that means, there is a possibility of common occurence reason of the accidents

##### Analyzing length of words and sentences
"""

print('--'*45); print('Get the length of each line, find the maximum length and print the maximum length line'); 
print('Length of line ranges from 64 to 672.'); print('--'*45)

# Get length of each line
nlp_data['line_length'] = nlp_data['Description'].str.len()

print('Minimum line length: {}'.format(nlp_data['line_length'].min()))
print('Maximum line length: {}'.format(nlp_data['line_length'].max()))
print('Line with maximum length: {}'.format(nlp_data[nlp_data['line_length'] == nlp_data['line_length'].max()]['Description'].values[0]))

fdist1 = nltk.FreqDist(tokens)
print(fdist1)

fdist1.most_common(20)

"""#### Observation :
* 1. The top 20 most common words are a combination of noun and verbs. However, nouns are more in number, indicating that what and who is repsonsible for accidents is more documented than exactly how 
* 2. Accidents are more related to hands (left and right) as compared to any other body part. This also shows that Metal , Mining and other industries are involving more manual labour or labour related to manual operating of machines
* 3. Employee and operator are the nouns that denotes who is facing the accident.

### Word Clouds
"""

wordcloud = WordCloud(width = 1500, height = 800, random_state=0, background_color='black', colormap='rainbow', \
                      min_font_size=5, max_words=300, collocations=False).generate(" ".join(nlp_data['Description'].values))
plt.figure(figsize=(15,10))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

plt.figure(figsize = (20,20)) # Text that is Not Sarcastic
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(nlp_data[nlp_data.Accident== 'V'].Description))
plt.imshow(wc , interpolation = 'bilinear')

plt.figure(figsize = (20,20)) # Text that is Not Sarcastic
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(nlp_data[nlp_data.Accident== 'I'].Description))
plt.imshow(wc , interpolation = 'bilinear')

plt.figure(figsize = (20,20)) # Text that is Not Sarcastic
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(nlp_data[nlp_data.Accident== 'II'].Description))
plt.imshow(wc , interpolation = 'bilinear')

plt.figure(figsize = (20,20)) # Text that is Not Sarcastic
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(nlp_data[nlp_data.Accident== 'III'].Description))
plt.imshow(wc , interpolation = 'bilinear')

plt.figure(figsize = (20,20)) # Text that is Not Sarcastic
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(nlp_data[nlp_data.Accident== 'IV'].Description))
plt.imshow(wc , interpolation = 'bilinear')

"""#### N Grams"""

# Function to calculate ngrams
def extract_ngrams(data, num):
  # Taking ngrams on Description column text and taking the value counts of each of the tokens
  words_with_count  = nltk.FreqDist(nltk.ngrams(data, num)).most_common(20) # taking top 30 most common words

  # Creating the dataframe the words and thier counts
  words_with_count = pd.DataFrame(words_with_count, columns=['Words', 'Count'])

  # Removing the brackets and commans
  words_with_count.Words = [' '.join(i) for i in words_with_count.Words]

  # words_with_count.index = [' '.join(i) for i in words_with_count.Words]
  words_with_count.set_index('Words', inplace=True) # setting the Words as index

  # Returns the dataframe which contains unique tokens ordered by their counts 
  return words_with_count

# Bi-Grams
bi_grams = extract_ngrams(tokens, 2)

# Printing the words with their counts
bi_grams[0:10]

bi_grams.sort_values(by='Count').plot.barh(color = 'blue', width = 0.8, figsize = (12,8));

# tri-Grams
tri_grams = extract_ngrams(tokens, 3)

# Printing the words with their counts
tri_grams[0:10]

tri_grams.sort_values(by='Count').plot.barh(color = 'blue', width = 0.8, figsize = (12,8));

# Dividing the tokens of male and female category from the description text
tokens_male = des_cleaning(' '.join(nlp_data[nlp_data.Gender=='Male']['Description'].sum().split()))
tokens_female = des_cleaning(' '.join(nlp_data[nlp_data.Gender=='Female']['Description'].sum().split()))

print('Total number of words in Metals category:', len(tokens_male))
print('Total number of words in Mining category:',len(tokens_female))

#Extracting bigrams on male category
bigrams_male = extract_ngrams(tokens_male, 2).reset_index()

# Extracting unigrams on female category
bigrams_female = extract_ngrams(tokens_female, 2).reset_index()

# Joining both the dataframes
bi_male_female = bigrams_male.join(bigrams_female, lsuffix='_Male', rsuffix='_Female')
print(bi_male_female)

sns.barplot(y = bi_male_female['Words_Female'], x = bi_male_female['Count_Female'], color='pink')

sns.barplot(y = bi_male_female['Words_Male'], x = bi_male_female['Count_Male'], color='skyblue')

"""### Glove embedding"""

from tqdm import tqdm
project_path =  "/content/drive/MyDrive/CAPSTONE PROJECT"
EMBEDDING_FILE = project_path + '/DataSet - glove.6B.50d.txt'
embeddings_index = {}
#EMBEDDING_FILE = "C://Users//sensen//OneDrive - HERE Global B.V//NLP//Open source libraries//Text summarization//glove.6B//glove.6B.50d.txt"
f = open(EMBEDDING_FILE, encoding="utf8")
for line in tqdm(f):
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

stop_words = stopwords.words('english')

# this function creates a normalized vector for the whole sentence
def sent2vec(s):
    words = str(s).lower()
    words = word_tokenize(words)
    words = [w for w in words if not w in stop_words]
    words = [w for w in words if w.isalpha()]
    M = []
    for w in words:
        try:
            M.append(embeddings_index[w])
        except:
            continue
    M = np.array(M)
    v = M.sum(axis=0)
    if type(v) != np.ndarray:
        return np.zeros(300)
    return v / np.sqrt((v ** 2).sum())

# create sentence GLOVE embeddings vectors using the above function for training and validation set
ind_glove_df = [sent2vec(x) for x in tqdm(nlp_data['Description'])]

ind_glove_df[0]

"""##### TF-IDF creation"""

from sklearn.feature_extraction.text import TfidfVectorizer


ind_tfidf_df = pd.DataFrame()
for i in [1,2,3]:
    vec_tfidf = TfidfVectorizer(max_features=10, norm='l2', stop_words='english', lowercase=True, use_idf=True, ngram_range=(i,i))
    X = vec_tfidf.fit_transform(nlp_data['Description']).toarray()
    tfs = pd.DataFrame(X, columns=["TFIDF_" + n for n in vec_tfidf.get_feature_names()])
    ind_tfidf_df = pd.concat([ind_tfidf_df.reset_index(drop=True), tfs.reset_index(drop=True)], axis=1)

ind_tfidf_df.head(3)

nlp_data.columns

"""##### Variable encoding"""

from sklearn.preprocessing import LabelEncoder

# Create Industry DataFrame
ind_featenc_df = pd.DataFrame()

# # Label encoding
nlp_data['Season'] = nlp_data['Season '].replace('Summer', 'aSummer').replace('Autumn', 'bAutumn').replace('Winter', 'cWinter').replace('Spring', 'dSpring')
ind_featenc_df['Season '] = LabelEncoder().fit_transform(nlp_data['Season ']).astype(np.int8)

nlp_data['Day'] = nlp_data['Day'].replace('Monday', 'aMonday').replace('Tuesday', 'bTuesday').replace('Wednesday', 'cWednesday').replace('Thursday', 'dThursday').replace('Friday', 'eFriday').replace('Saturday', 'fSaturday').replace('Sunday', 'gSunday')
ind_featenc_df['Day'] = LabelEncoder().fit_transform(nlp_data['Day']).astype(np.int8)

ind_featenc_df['Accident'] = LabelEncoder().fit_transform(nlp_data['Accident']).astype(np.int8)
ind_featenc_df['Potential Accident Level'] = LabelEncoder().fit_transform(nlp_data['PotentialAccident']).astype(np.int8)

ind_featenc_df['Gender'] = LabelEncoder().fit_transform(nlp_data['Gender']).astype(np.int8)
ind_featenc_df['Industry Sector'] = LabelEncoder().fit_transform(nlp_data['Industry Sector']).astype(np.int8)

ind_featenc_df['Month'] = LabelEncoder().fit_transform(nlp_data['Month']).astype(np.int8)
ind_featenc_df['Year'] = LabelEncoder().fit_transform(nlp_data['Year']).astype(np.int8)
ind_featenc_df['CriticalRisk'] = LabelEncoder().fit_transform(nlp_data['CriticalRisk']).astype(np.int8)
ind_featenc_df['Employeetype'] = LabelEncoder().fit_transform(nlp_data['Employeetype']).astype(np.int8)
# ind_featenc_df['NewDescription'] = nlp_data['Description']

ind_featenc_df

# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(ind_featenc_df['Accident'])
dummy_y

# Consider only top 30 GLOVE features
ind_feat_df = ind_featenc_df.join(pd.DataFrame(ind_glove_df).iloc[:,0:30].reset_index(drop=True))

ind_feat_df.head(3)

"""#### Splitting dataset"""

X = ind_feat_df.drop(['Accident','Potential Accident Level'], axis = 1) # Considering all Predictors
y = ind_feat_df['Accident']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 1, stratify = y)

X_train, X_test, y_train_dummy, y_test_dummy = train_test_split(X, dummy_y, test_size = 0.20, random_state = 1, stratify = y)

print('X_train shape : ({0},{1})'.format(X_train.shape[0], X_train.shape[1]))
print('y_train shape : ({0},)'.format(y_train.shape[0]))
print('X_test shape : ({0},{1})'.format(X_test.shape[0], X_test.shape[1]))
print('y_test shape : ({0},)'.format(y_test.shape[0]))

# Display old accident level counts
ind_feat_df['Accident'].value_counts()

"""#### SMOTE Oversampling"""

#pip install -U imbalanced-learn

from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=1)
X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)
df_smote = pd.concat([pd.DataFrame(X_train_smote), pd.DataFrame(y_train_smote)], axis=1)

X_train_smote

y_train_smote

y_train_smote.value_counts()

"""#### All the classes are now balanced"""

X_train_smote.shape

y_train_smote

"""#### Observation :
* 1 .Oversampling has been done for oth X and y and their shapes are same
"""

# convert integers to dummy variables (i.e. one hot encoded)
y_train_smote_dummy = np_utils.to_categorical(y_train_smote)
y_train_smote_dummy

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline


# Transform independent features
scaler_X = StandardScaler()#StandardScaler()
pipeline = Pipeline(steps=[('s', scaler_X)])
X_train.iloc[:,:6] = pipeline.fit_transform(X_train.iloc[:,:6]) # Scaling only first 6 feautres

X_test.iloc[:,:6] = pipeline.fit_transform(X_test.iloc[:,:6]) # Scaling only first 6 feautres

X_train.head(3)

"""####  USing PCA to capture variation in data at 90%"""

# generating the covariance matrix and the eigen values for the PCA analysis
cov_matrix = np.cov(X_train.T) # the relevanat covariance matrix
print('Covariance Matrix \n%s', cov_matrix)

#generating the eigen values and the eigen vectors
e_vals, e_vecs = np.linalg.eig(cov_matrix)
print('Eigenvectors \n%s' %e_vecs)
print('\nEigenvalues \n%s' %e_vals)

# the "cumulative variance explained" analysis 
tot = sum(e_vals)
var_exp = [( i /tot ) * 100 for i in sorted(e_vals, reverse=True)]
cum_var_exp = np.cumsum(var_exp)
print("Cumulative Variance Explained", cum_var_exp)

# Plotting the variance expalained by the principal components and the cumulative variance explained.
plt.figure(figsize=(20 , 5))
plt.bar(range(1, e_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')
plt.step(range(1, e_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')
plt.ylabel('Explained Variance Ratio')
plt.xlabel('Principal Components')
plt.legend(loc = 'best')
plt.tight_layout()
plt.show()

from sklearn.decomposition import PCA

# Capturing 90% variance of the data
pca = PCA(n_components = 0.90)
X_train_reduced = pca.fit_transform(X_train)
X_test_reduced = pca.transform(X_test)

print(X_train_reduced.shape)
print(X_test_reduced.shape)

"""### Model 1 : BiDirectional LSTM With Categorical Variables and Balanced Data only. 
*** Embedding used - None
*** No Text data has been used (Description Column). Hence no embedding has been used here
"""

y_train_smote

X_train_smote.columns

# Select input and output features
X_cat = X_train_smote.drop([0, 1, 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29], axis = 1) # Considering all Predictors
y_cat = y_train_smote

# Encode labels in column 'Accident Level'.
y_cat = LabelEncoder().fit_transform(y_cat)

# Divide our data into testing and training sets:
X_cat_train, X_cat_test, y_cat_train, y_cat_test = train_test_split(X_cat, y_cat, test_size = 0.20, random_state = 1, stratify = y_cat)

print('X_cat_train shape : ({0})'.format(X_cat_train.shape[0]))
print('y_cat_train shape : ({0},)'.format(y_cat_train.shape[0]))
print('X_cat_test shape : ({0})'.format(X_cat_test.shape[0]))
print('y_cat_test shape : ({0},)'.format(y_cat_test.shape[0]))

# Convert both the training and test labels into one-hot encoded vectors:
y_cat_train = np_utils.to_categorical(y_cat_train)
y_cat_test = np_utils.to_categorical(y_cat_test)

# Variable transformation using StandardScaler
scaler_X = StandardScaler()#StandardScaler()
X_cat_train.iloc[:,:6] = scaler_X.fit_transform(X_cat_train.iloc[:,:6]) # Scaling only first 6 feautres

X_cat_test.iloc[:,:6] = scaler_X.fit_transform(X_cat_test.iloc[:,:6]) # Scaling only first 6 feautres

class Metrics(tf.keras.callbacks.Callback):

    def __init__(self, validation_data=()):
        super().__init__()
        self.validation_data = validation_data

    def on_train_begin(self, logs={}):
        self.val_f1s = []
        self.val_recalls = []
        self.val_precisions = []

    def on_epoch_end(self, epoch, logs={}):
        xVal, yVal, target_type = self.validation_data
        if target_type == 'multi_class':
          val_predict_classes = model.predict_classes(xVal, verbose=0) # Multiclass
        else:
          val_predict_classes = (np.asarray(self.model.predict(xVal))).round() # Multilabel
        
        
        val_targ = yVal

        _val_f1 = f1_score(val_targ, val_predict_classes, average='micro')
        _val_recall = recall_score(val_targ, val_predict_classes, average='micro')
        _val_precision = precision_score(val_targ, val_predict_classes, average='micro')
        self.val_f1s.append(_val_f1)
        self.val_recalls.append(_val_recall)
        self.val_precisions.append(_val_precision)
        #print("— train_f1: %f — train_precision: %f — train_recall %f" % (_val_f1, _val_precision, _val_recall))
        return


# Select input and output features
X_cat_unbl = ind_featenc_df.drop(['Accident','Potential Accident Level'], axis = 1)
y_cat = nlp_data['Accident']

# Encode labels in column 'Accident Level'.
y_cat = LabelEncoder().fit_transform(y_cat)

# Divide our data into testing and training sets:
X_cat_train, X_cat_test, y_cat_train, y_cat_test = train_test_split(X_cat_unbl, y_cat, test_size = 0.20, random_state = 1, stratify = y_cat)

print('X_cat_train shape : ({0})'.format(X_cat_train.shape[0]))
print('y_cat_train shape : ({0},)'.format(y_cat_train.shape[0]))
print('X_cat_test shape : ({0})'.format(X_cat_test.shape[0]))
print('y_cat_test shape : ({0},)'.format(y_cat_test.shape[0]))

# Convert both the training and test labels into one-hot encoded vectors:
y_cat_train = np_utils.to_categorical(y_cat_train)
y_cat_test = np_utils.to_categorical(y_cat_test)

# Variable transformation using StandardScaler
scaler_X = StandardScaler()#StandardScaler()
X_cat_train.iloc[:,:6] = scaler_X.fit_transform(X_cat_train.iloc[:,:6]) # Scaling only first 6 feautres

X_cat_test.iloc[:,:6] = scaler_X.fit_transform(X_cat_test.iloc[:,:6]) # Scaling only first 6 feautres


#Importing necessary libraries
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

## Preprocessing the text

##Changing to lower case and substituiting numeric and special characters with nothing

nlp_data['Description'] =nlp_data['Description'].apply(lambda x: x.lower())
nlp_data['Description'] = nlp_data['Description'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))

## Removing Stop words

stop = set(stopwords.words('english'))
punctuation = list(string.punctuation)
stop.update(punctuation)

#Removing the stopwords from text
def remove_stopwords(text):
    final_text = []
    for i in text.split():
        if i.strip().lower() not in stop:
            final_text.append(i.strip())
    return " ".join(final_text)

#Apply function on review column
nlp_data['Description']=nlp_data['Description'].apply(remove_stopwords)

# The first step in word embeddings is to convert the words into thier corresponding numeric indexes.
# Select input and output features
X_text = nlp_data['Description']
y_text = nlp_data['Accident']

# Encode labels in column 'Accident Level'.
y_text = LabelEncoder().fit_transform(y_text)

# Divide our data into testing and training sets:
X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(X_text, y_text, test_size = 0.20, random_state = 1, stratify = y_text)

print('X_text_train shape : ({0})'.format(X_text_train.shape[0]))
print('y_text_train shape : ({0},)'.format(y_text_train.shape[0]))
print('X_text_test shape : ({0})'.format(X_text_test.shape[0]))
print('y_text_test shape : ({0},)'.format(y_text_test.shape[0]))

# Convert both the training and test labels into one-hot encoded vectors:
y_text_train = np_utils.to_categorical(y_text_train)
y_text_test = np_utils.to_categorical(y_text_test)

# The first step in word embeddings is to convert the words into thier corresponding numeric indexes.
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_text_train)

X_text_train = tokenizer.texts_to_sequences(X_text_train)
X_text_test = tokenizer.texts_to_sequences(X_text_test)

# Sentences can have different lengths, and therefore the sequences returned by the Tokenizer class also consist of variable lengths.
# We need to pad the our sequences using the max length.
vocab_size = len(tokenizer.word_index) + 1
print("vocab_size:", vocab_size)

maxlen = 100

X_text_train = pad_sequences(X_text_train, padding='post', maxlen=maxlen)
X_text_test = pad_sequences(X_text_test, padding='post', maxlen=maxlen)

# We need to load the built-in GloVe word embeddings
embedding_size = 50
embeddings_dictionary = dict()


glove_file = open(EMBEDDING_FILE, encoding="utf8")

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = np.asarray(records[1:], dtype='float32')
    embeddings_dictionary[word] = vector_dimensions

glove_file.close()

embedding_matrix = np.zeros((vocab_size, embedding_size))

for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

len(embeddings_dictionary.values())

input_1 = Input(shape=(maxlen,))
input_1.shape[1]


"""### Model_4 with multiple inputs

* Model takes both categorical and text data as input
* The Model uses glove embedding with dimension 50 as for Text data which is the Description Column
* The Model also takes the categorical data converted into one hot encoding
* The Model is basically made in three parts 
   ** First architecture uses BiDirectional LSTM with glove embeddings on Description column 
   ** Second Architecture uses BiDirectional LSTM with Categorical inputs 
   ** Thrid architecture concatenates the output of the above two architectures
* This model is a deep neural network of sequential type
"""

# Initialize the random number generator
import random
random.seed(0)

input_1 = Input(shape=(maxlen,))
embedding_layer   = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(input_1)
LSTM_Layer_1      = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)
max_pool_layer_1  = GlobalMaxPool1D()(LSTM_Layer_1)
drop_out_layer_1  = Dropout(0.5, input_shape = (256,))(max_pool_layer_1)
dense_layer_1     = Dense(128, activation = 'relu')(drop_out_layer_1)
drop_out_layer_2  = Dropout(0.5, input_shape = (128,))(dense_layer_1)
dense_layer_2     = Dense(64, activation = 'relu')(drop_out_layer_2)
drop_out_layer_3  = Dropout(0.5, input_shape = (64,))(dense_layer_2)

dense_layer_3     = Dense(32, activation = 'relu')(drop_out_layer_3)
drop_out_layer_4  = Dropout(0.5, input_shape = (32,))(dense_layer_3)

dense_layer_4     = Dense(10, activation = 'relu')(drop_out_layer_4)
drop_out_layer_5 = Dropout(0.5, input_shape = (10,))(dense_layer_4)

##########################################################
param = 1e-4

input_2 = Input(shape=(X_cat_train.shape[1],))
dense_layer_5       = Dense(10, input_dim=X_cat_train.shape[1], activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),
                      kernel_constraint=unit_norm())(input_2)
drop_out_layer_6    = Dropout(0.2)(dense_layer_5)
batch_norm_layer_1  = BatchNormalization()(drop_out_layer_6)
dense_layer_6       = Dense(10, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param), 
                            kernel_constraint=unit_norm())(batch_norm_layer_1)
drop_out_layer_7   = Dropout(0.5)(dense_layer_6)
batch_norm_layer_2 = BatchNormalization()(drop_out_layer_7)

concat_layer        = Concatenate()([drop_out_layer_5, batch_norm_layer_2])
dense_layer_7       = Dense(10, activation='relu')(concat_layer)
output  = Dense(5, activation='softmax')(dense_layer_7)
model_4   = Model(inputs=[input_1, input_2], outputs=output)

# compile the keras model
#opt = optimizers.Adamax(lr=0.01)
opt = SGD(lr=0.001, momentum=0.9)
model_4.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])

print(model_4.summary())

# Use earlystopping
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)
rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)

target_type = 'multi_label'
metrics = Metrics(validation_data=([X_text_train, X_cat_train], y_cat_train, target_type))

# fit the keras model on the dataset
training_history = model_4.fit([X_text_train, X_cat_train], y_cat_train, epochs=100, batch_size=8, verbose=1, validation_data=([X_text_test, X_cat_test], y_cat_test), callbacks=[rlrp, metrics])

# evaluate the keras model
_, train_accuracy = model_4.evaluate([X_text_train, X_cat_train], y_cat_train, batch_size=8, verbose=0)
_, test_accuracy = model_4.evaluate([X_text_test, X_cat_test], y_cat_test, batch_size=8, verbose=0)

print('Train accuracy: %.2f' % (train_accuracy*100))
print('Test accuracy: %.2f' % (test_accuracy*100))

# get the accuracy, precision, recall, f1 score from model
def get_classification_metrics(model, X_test, y_test, target_type):
  
  # predict probabilities for test set
  yhat_probs = model.predict(X_test, verbose=0) # Multiclass

  # predict crisp classes for test set
  if target_type == 'multi_class':
    yhat_classes = model.predict_classes(X_test, verbose=0) # Multiclass
  else:
    yhat_classes = (np.asarray(model.predict(X_test))).round() # Multilabel

  # reduce to 1d array
  yhat_probs = yhat_probs[:, 0]

  # accuracy: (tp + tn) / (p + n)
  accuracy = accuracy_score(y_test, yhat_classes)

  # precision tp / (tp + fp)
  precision = precision_score(y_test, yhat_classes, average='micro')

  # recall: tp / (tp + fn)
  recall = recall_score(y_test, yhat_classes, average='micro')

  # f1: 2 tp / (2 tp + fp + fn)
  f1 = f1_score(y_test, yhat_classes, average='micro')

  return accuracy, precision, recall, f1

accuracy, precision, recall, f1 = get_classification_metrics(model_4, [X_text_test, X_cat_test], y_cat_test, target_type)
print('Accuracy: %f' % accuracy)
print('Precision: %f' % precision)
print('Recall: %f' % recall)
print('F1 score: %f' % f1)

epochs = range(len(training_history.history['loss'])) # Get number of epochs

# plot loss learning curves
plt.plot(epochs, training_history.history['loss'], label = 'train')
plt.plot(epochs, training_history.history['val_loss'], label = 'test')
plt.legend(loc = 'upper right')
plt.title ('Training and validation loss')

# plot accuracy learning curves
plt.plot(epochs, training_history.history['acc'], label = 'train')
plt.plot(epochs, training_history.history['val_acc'], label = 'test')
plt.legend(loc = 'upper right')
plt.title ('Training and validation accuracy')


# Defining a function which quickly test the fit of 6 different models on the dataset

from keras.models import load_model
model_4.save('chatbot_model.h5')
m = load_model('chatbot_model.h5')
m

"""Model 4 has the highest accuracy and has been pickled for future use.

